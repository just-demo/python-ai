# python-ai

```
ollama serve
ollama stop

ollama ls
ollama run llama3.2

ollama pull llama3.2
ollama rm llama3.2
```

http://localhost:11434/